{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13611596,"sourceType":"datasetVersion","datasetId":8650079}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom collections import Counter\n\n# Load separately\ntrain = pd.read_csv(\"/kaggle/input/cicds-unsw-alligned-dataset/CICIDS_aligned_train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/cicds-unsw-alligned-dataset/CICIDS_aligned_test.csv\")\n\nprint(\"Train:\", train.shape, \" Test:\", test.shape)\n\n# Clean duplicate label columns\nfor df in [train, test]:\n    for col in [\"Label\", \"raw_label\", \"Attack\", \"attack_label\", \"class\", \"Category\"]:\n        if col in df.columns:\n            df.drop(columns=[col], inplace=True, errors=\"ignore\")\n\n# Clean attack_cat safely\nfor df in [train, test]:\n    df[\"attack_cat\"] = pd.to_numeric(df[\"attack_cat\"], errors=\"coerce\")\n    df.dropna(subset=[\"attack_cat\"], inplace=True)\n    df[\"attack_cat\"] = df[\"attack_cat\"].astype(int)\n\nprint(\"âœ… Labels cleaned\")\n\n# ---------------------------\n# Controlled Class Sampling\n# ---------------------------\n\nfrom collections import Counter\n\ndef balance(df):\n    counts = Counter(df[\"attack_cat\"])\n    print(\"Before:\", counts)\n\n    balanced = []\n\n    for cls, cnt in counts.items():\n\n        # Very large classes â†’ keep 80k\n        if cnt > 80000:\n            balanced.append(df[df[\"attack_cat\"] == cls].sample(80000, random_state=42))\n\n        # Medium classes â†’ keep 40k\n        elif cnt > 40000:\n            balanced.append(df[df[\"attack_cat\"] == cls].sample(40000, random_state=42))\n\n        # Small classes â†’ keep all (important!)\n        else:\n            balanced.append(df[df[\"attack_cat\"] == cls])\n\n    df_bal = pd.concat(balanced).sample(frac=1, random_state=42).reset_index(drop=True)\n    print(\"After:\", Counter(df_bal[\"attack_cat\"]))\n    return df_bal\ntrain_bal = balance(train)\ntest_bal  = balance(test)\n\n# Save balanced sets\ntrain_bal.to_csv(\"CICIDS2018_train_balanced_alligned.csv\", index=False)\ntest_bal.to_csv(\"CICIDS2018_test_balanced_alligned.csv\", index=False)\n\nprint(\"âœ… Saved:\")\nprint(\"Train:\", train_bal.shape, \" Test:\", test_bal.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:51:04.355528Z","iopub.execute_input":"2025-11-04T16:51:04.356210Z","iopub.status.idle":"2025-11-04T16:53:50.416819Z","shell.execute_reply.started":"2025-11-04T16:51:04.356170Z","shell.execute_reply":"2025-11-04T16:53:50.415672Z"}},"outputs":[{"name":"stdout","text":"Train: (5327625, 133)  Test: (1331907, 133)\nâœ… Labels cleaned\nBefore: Counter({5: 4263206, 2: 620764, 3: 157254, 0: 115628, 4: 94786, 8: 75238, 1: 638, 6: 68, 7: 43})\nAfter: Counter({0: 80000, 3: 80000, 4: 80000, 5: 80000, 2: 80000, 8: 40000, 1: 638, 6: 68, 7: 43})\nBefore: Counter({5: 1065802, 2: 155191, 3: 39314, 0: 28907, 4: 23697, 8: 18810, 1: 159, 6: 17, 7: 10})\nAfter: Counter({2: 80000, 5: 80000, 3: 39314, 0: 28907, 4: 23697, 8: 18810, 1: 159, 6: 17, 7: 10})\nâœ… Saved:\nTrain: (440749, 132)  Test: (270914, 132)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport joblib\nfrom collections import Counter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# ===== LOAD =====\ntrain = pd.read_csv(\"/kaggle/working/CICIDS2018_train_balanced_alligned.csv\")\ntest  = pd.read_csv(\"/kaggle/working/CICIDS2018_test_balanced_alligned.csv\")\n\nX_train = train.drop(columns=[\"attack_cat\"])\ny_train = train[\"attack_cat\"]\n\nX_test  = test.drop(columns=[\"attack_cat\"])\ny_test  = test[\"attack_cat\"]\n\nprint(\"Train:\", train.shape, \"| Test:\", test.shape)\nprint(\"Class distribution:\", Counter(y_train))\n\n# ===== SCALE =====\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\njoblib.dump(scaler, \"scaler.pkl\")\n\n# ===== CLASS WEIGHTS =====\nclasses = sorted(y_train.unique())\ncw = compute_class_weight(\"balanced\", classes=classes, y=y_train)\nclass_weights = {classes[i]: float(cw[i]) for i in range(len(cw))}\n\n# ===== MODELS =====\nmodel_lgb = LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    class_weight=class_weights,\n    random_state=42\n)\n\nmodel_xgb = XGBClassifier(\n    objective=\"multi:softprob\",\n    num_class=len(classes),\n    tree_method=\"hist\",\n    n_estimators=300,\n    max_depth=7,\n    learning_rate=0.05,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    eval_metric=\"mlogloss\",\n    random_state=42\n)\n\nmodel_cat = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=6,\n    class_weights=list(cw),\n    verbose=False,\n    loss_function=\"MultiClass\"\n)\n\nmodel_rf = RandomForestClassifier(\n    n_estimators=260,\n    max_depth=14,\n    class_weight=\"balanced\",\n    random_state=42,\n    n_jobs=-1\n)\n\nprint(\"\\nTraining models...\")\nmodel_lgb.fit(X_train, y_train)\nmodel_xgb.fit(X_train, y_train)\nmodel_cat.fit(X_train, y_train)\nmodel_rf.fit(X_train, y_train)\n\njoblib.dump(model_lgb, \"lgb.pkl\")\njoblib.dump(model_xgb, \"xgb.pkl\")\njoblib.dump(model_cat, \"cat.pkl\")\njoblib.dump(model_rf, \"rf.pkl\")\n\nprint(\"\\nEvaluating ensemble...\")\np1 = model_lgb.predict_proba(X_test)\np2 = model_xgb.predict_proba(X_test)\np3 = model_cat.predict_proba(X_test)\np4 = model_rf.predict_proba(X_test)\n\np_avg = (p1 + p2 + p3 + p4) / 4.0\ny_pred = p_avg.argmax(axis=1)\n\nprint(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"ðŸŽ¯ Macro F1:\", f1_score(y_test, y_pred, average=\"macro\"))\nprint(\"\\n\", classification_report(y_test, y_pred, zero_division=0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T16:59:39.865990Z","iopub.execute_input":"2025-11-04T16:59:39.866470Z","iopub.status.idle":"2025-11-04T17:11:08.881967Z","shell.execute_reply.started":"2025-11-04T16:59:39.866436Z","shell.execute_reply":"2025-11-04T17:11:08.880752Z"}},"outputs":[{"name":"stdout","text":"Train: (440749, 132) | Test: (270914, 132)\nClass distribution: Counter({0: 80000, 3: 80000, 4: 80000, 5: 80000, 2: 80000, 8: 40000, 1: 638, 6: 68, 7: 43})\n\nTraining models...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057808 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 13708\n[LightGBM] [Info] Number of data points in the train set: 440749, number of used features: 69\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Info] Start training from score -2.197225\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\nEvaluating ensemble...\nâœ… Accuracy: 0.900809851096658\nðŸŽ¯ Macro F1: 0.8142349100460721\n\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     28907\n           1       0.51      0.97      0.67       159\n           2       1.00      1.00      1.00     80000\n           3       1.00      1.00      1.00     39314\n           4       0.47      0.92      0.62     23697\n           5       0.97      0.69      0.80     80000\n           6       0.54      0.76      0.63        17\n           7       0.43      1.00      0.61        10\n           8       1.00      1.00      1.00     18810\n\n    accuracy                           0.90    270914\n   macro avg       0.77      0.93      0.81    270914\nweighted avg       0.94      0.90      0.91    270914\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport joblib\n\ncic = pd.read_csv(\"/kaggle/working/CICIDS2018_train_balanced_alligned.csv\")\n\nfeature_list = [c for c in cic.columns if c != \"attack_cat\"]\njoblib.dump(feature_list, \"aligned_feature_list.pkl\")\n\nprint(\"âœ… Saved aligned feature list:\", len(feature_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T17:52:32.611197Z","iopub.execute_input":"2025-11-04T17:52:32.612543Z","iopub.status.idle":"2025-11-04T17:52:37.771364Z","shell.execute_reply.started":"2025-11-04T17:52:32.612484Z","shell.execute_reply":"2025-11-04T17:52:37.770220Z"}},"outputs":[{"name":"stdout","text":"âœ… Saved aligned feature list: 131\n","output_type":"stream"}],"execution_count":6}]}